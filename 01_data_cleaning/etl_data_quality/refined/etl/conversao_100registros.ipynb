{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81174313-5aa2-444b-854c-678e2c713d98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/10/13 20:20:30 WARN Utils: Your hostname, tatiane-Inspiron-3583, resolves to a loopback address: 127.0.1.1; using 192.168.1.52 instead (on interface wlo1)\n",
      "25/10/13 20:20:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/13 20:20:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 41212)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/socketserver.py\", line 318, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.12/socketserver.py\", line 349, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.12/socketserver.py\", line 362, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.12/socketserver.py\", line 761, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/tatiane/spark/lib/python3.12/site-packages/pyspark/accumulators.py\", line 299, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/tatiane/spark/lib/python3.12/site-packages/pyspark/accumulators.py\", line 271, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/home/tatiane/spark/lib/python3.12/site-packages/pyspark/accumulators.py\", line 275, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tatiane/spark/lib/python3.12/site-packages/pyspark/serializers.py\", line 597, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"test_session\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6af6ae1c-d70a-4e69-975d-324801a7031d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo inicial da execucao: 2025-10-13 20:20:41.809066\n",
      "User: tatiane\n",
      "Node: tatiane-Inspiron-3583\n"
     ]
    }
   ],
   "source": [
    "import sys, os, time, getpass\n",
    "\n",
    "sys.path.append(\"/home/tatiane/lib/\")\n",
    "\n",
    "import pessoal\n",
    "from pessoal import *\n",
    "#print('AppID: ', sc.applicationId)\n",
    "\n",
    "# Pedir ao spark para exibir só erros, escondendo avisos (WARN) e infos\n",
    "# O correto é resolver a causa como, por exemplo: ordenados = Window.partitionBy().orderBy(dfs[i].columns[0])\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc59b0e-36a6-498a-884a-c46a9f9cf30f",
   "metadata": {},
   "source": [
    "## Conversão de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "062df4bb-0167-4bcd-9aff-90656141d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter Excel para CSV\n",
    "df_pandas = pd.read_excel(\"/home/tatiane/Downloads/base_dados_suja.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "212d427c-bbbe-4711-abd6-f493e30acaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de linhas: 100\n",
      "Número de colunas: 4\n"
     ]
    }
   ],
   "source": [
    "num_linhas, num_colunas = df_pandas.shape\n",
    "print(f\"Número de linhas: {num_linhas}\")\n",
    "print(f\"Número de colunas: {num_colunas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33519367-b1e2-4f0a-86a9-cda098030d31",
   "metadata": {},
   "source": [
    "### Criação de identificador único"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8863397e-976a-40ca-8489-1a90b8285c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createId(df, columnName='id_unico', beginAt=0):\n",
    "    df = df.copy()\n",
    "    df[columnName] = range(beginAt + 1, beginAt + len(df) + 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f7ae7f6-328e-491e-8d36-bf5dadbde358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mínimo: 1\n",
      "Máximo: 100\n"
     ]
    }
   ],
   "source": [
    "df_pandas = createId(df_pandas, 'id_unico')\n",
    "\n",
    "min_val = df_pandas['id_unico'].min()\n",
    "max_val = df_pandas['id_unico'].max()\n",
    "\n",
    "print(\"Mínimo:\", min_val)\n",
    "print(\"Máximo:\", max_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dae0345-7489-45e1-ad21-546b93bd50ad",
   "metadata": {},
   "source": [
    "## Converter Excel para CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67cd2fc1-3e5e-4900-bde4-f32058c9106f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csv_path = \"/home/tatiane/Downloads/base_dados_suja.csv\"\n",
    "df_pandas.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "245fc9c5-89fa-4182-8912-7a32cb1d0a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qtd. registros: 100 | Quantidade de colunas:  5\n",
      "root\n",
      " |-- nome: string (nullable = true)\n",
      " |-- data_nascimento: string (nullable = true)\n",
      " |-- municipio_nascimento: string (nullable = true)\n",
      " |-- sexo: string (nullable = true)\n",
      " |-- id_unico: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "pessoal.completudeSchema(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d4c3f84-bd2f-42c7-b604-9b9f3dd0ffa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+--------------------+----+--------+\n",
      "|                nome|  data_nascimento|municipio_nascimento|sexo|id_unico|\n",
      "+--------------------+-----------------+--------------------+----+--------+\n",
      "|     _ok Pedro Gomes|15/fevereiro/1997|            Curitiba|   M|       1|\n",
      "| Camila _ok Monteiro|       2001/07/16|      Rio de Janeiro|   M|       2|\n",
      "|_ok Ricardo Teixeira|12/fevereiro/1973|            Salvador|   F|       3|\n",
      "|  Paula Azevedo NULL|       1990/06/15|      Rio de Janeiro|   F|       4|\n",
      "|Fernanda Martins ...|    08/junho/2010|      Belo Horizonte|   M|       5|\n",
      "|         Rafael Lima|    08/junho/1976|        Porto Alegre|   M|       6|\n",
      "|     NULL Sofia Melo|       20/11/2003|            Curitiba|   M|       7|\n",
      "|         André Pinto|  13/outubro/2008|           Fortaleza|   F|       8|\n",
      "|    123 Bruno Santos|       07/01/1971|      Rio de Janeiro|   M|       9|\n",
      "|    @ Carla Nogueira|       20/10/1997|              Manaus|   F|      10|\n",
      "|    Maria Silva temp|       1985/11/09|      Rio de Janeiro|   M|      11|\n",
      "|  temp Beatriz Costa|06/fevereiro/2002|      Belo Horizonte|   M|      12|\n",
      "|    Ricardo Teixeira|  16/janeiro/1978|              Recife|   F|      13|\n",
      "|teste Camila Mont...|       04/08/1997|            Curitiba|   M|      14|\n",
      "|      Carla Nogueira|15/fevereiro/2010|              Manaus|   M|      15|\n",
      "|     ### Maria Silva| 24/dezembro/1976|        Porto Alegre|   M|      16|\n",
      "|Paula sem dado Az...| 10/setembro/1974|              Recife|   M|      17|\n",
      "|       Rafael @ Lima|       1994/12/27|           São Paulo|   F|      18|\n",
      "|Aline Barbosa sem...|       2009/08/26|              Manaus|   M|      19|\n",
      "|          João Souza|       26/05/1978|      Rio de Janeiro|   M|      20|\n",
      "+--------------------+-----------------+--------------------+----+--------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65a45f9-693e-43a3-b56d-79821f235c1b",
   "metadata": {},
   "source": [
    "### Escrita da base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa254d45-c371-4dc6-8067-3817d8cc0eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_path = '/home/tatiane/Documentos/tratamento_dados/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ada90f78-d1be-4611-b66a-166d2d76e38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/home/tatiane/Documentos/tratamento_dados/tratamento_dados.csv',\n",
       " 1.8179035329994804,\n",
       " 1.8559890420001466)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writeSingleCsv(df, write_path)\n",
    "# arquivo foi renomeado para: base_convertida.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74f3bb9-4e8c-4938-b0c6-fda24d7eb5a5",
   "metadata": {},
   "source": [
    "### Finalização do notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af0283f3-0acb-4b70-9ea0-694141e783fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de execucao ate este ponto: 0:00:19.171610\n"
     ]
    }
   ],
   "source": [
    "executionTime()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Spark Venv)",
   "language": "python",
   "name": "meu_ambiente"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
